---
title: "CAFO NH3 Analysis"
output: pdf_document
---

Relevant Links: 

-EPA NEI: https://www.epa.gov/air-emissions-inventories/2017-national-emissions-inventory-nei-data 
-EPA NPDES: https://www.epa.gov/npdes/npdes-cafo-regulations-implementation-status-reports
-EPA AQS: https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw
-IMPROVE Network: http://views.cira.colostate.edu/fed/QueryWizard/Default.aspx
-USDA Agriculture Census: https://www.nass.usda.gov/AgCensus/index.php

```{r}
require(ncdf4)
require(lubridate)
require(ggplot2)
```


```{r}
lon.lim <- c(-125.5,-65.5)
lat.lim <- c(25.5,50.5) 
months <- c('jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec')
years <- 2003:2015
```

```{r}
all.data <- c()
for (j in 1:length(years)) {
  for (i in 1:12) {
    file <- paste("AIRS_NH3_VMR_", months[i], ".", toString(years[j]), ".V3.nc", sep="")  
    nc_ds <- nc_open(file)
    var1 <- ncvar_get(nc_ds, "nh3_vmr", collapse_degen=TRUE)
    dim_depth <- ncvar_get(nc_ds, "press_level")
    dim_lon <- ncvar_get(nc_ds, "lon")
    dim_lat <- ncvar_get(nc_ds, "lat")
    dim_time <- ncvar_get(nc_ds, "time")
    t_units <- ncatt_get(nc_ds, "time", "units")
    t_ustr <- strsplit(t_units$value, " ")
    t_dstr <- strsplit(unlist(t_ustr)[3], "-")
    date <- ymd(t_dstr) + dseconds(dim_time)
    coords <- as.matrix(expand.grid(dim_depth,dim_lon, dim_lat,  date))
    nc_df <- data.frame(cbind(coords, var1))
    names(nc_df) <- c("pressure","lon", "lat",  "time", "nh3")
    nc_df <- na.omit(nc_df)
    for (k in c(1,2,3,5)) 
      nc_df[,k] <- as.numeric(nc_df[,k])
    nc_df <- nc_df[nc_df$pressure == 918.1261,]
    nc_df <- nc_df[nc_df$lon >= lon.lim[1] & nc_df$lon <= lon.lim[2],]
    nc_df <- nc_df[nc_df$lat >= lat.lim[1] & nc_df$lat <= lat.lim[2],]
    all.data <- rbind(all.data, nc_df)
  }
}

```


```{r}
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
points(nc_df$lon, nc_df$lat, cex=nc_df$nh3/max(nc_df$nh3))
```
```{r}

all.locs <- unique(all.data[,2:3])
baseline <- rep(NA, nrow(all.locs))
slopes <- rep(NA, nrow(all.locs))
for (i in 1:nrow(all.locs)) {
  loc <- unlist(as.vector(all.locs[i,]))
  loc.data <- all.data[((all.data[,2]==loc[1]) & (all.data[,3]==loc[2])),]
  if (nrow(loc.data) > 4) {
  months <- month(loc.data[,4])
  years <- year(loc.data[,4])-2003
  time <- months + 12*years
  fit <- lm(loc.data[,5]~time)$coef
  baseline[i] <- fit[1]
  slopes[i] <- fit[2]
  }
}
```

```{r}
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}

all.slopes <- na.omit(cbind(all.locs, slopes, baseline))
baseline1 <- all.slopes[,4]
baseline1 <- baseline1 / max(baseline1)
slopes1 <- all.slopes[,3]
slopes.cex <- abs(slopes1)
slopes.cex <- slopes.cex/max(slopes.cex)
points(all.slopes[,1], all.slopes[,2], cex=slopes.cex*3, col=2+(slopes1>0), pch=19)
```

```{r}
la <- 844
loc <- unlist(all.slopes[844,1:2])
loc.data <- all.data[((all.data[,2]==loc[1]) & (all.data[,3]==loc[2])),]
months <- month(loc.data[,4])
years <- year(loc.data[,4])-2003
time <- months + 12*years
fit <- lm(loc.data[,5]~time)$coef
```


```{r}
all.iasi <- c()
for (i in 6:8) {
  for (j in 1:(31-(i==6))) {
    if (i==6 & j==7)
      next
    m <- toString(j)
    if (nchar(m)==1) 
      m <- paste(0,m, sep="")
    file <- paste('IASI_METOPB_L2_NH3_20170', i, m,'_ULB-LATMOS_V3.1.0.nc', sep="")
    nc_ds <- nc_open(file)
    dim_lon <- ncvar_get(nc_ds, "longitude")
    dim_lat <- ncvar_get(nc_ds, "latitude")
    dim_time <- ncvar_get(nc_ds, "time_in_day")
    var1 <- ncvar_get(nc_ds, "nh3_total_column")
    nc_df <- data.frame(lon=dim_lon, lat=dim_lat, time=dim_time, nh3=var1)
    nc_df <- nc_df[nc_df$lon >= lon.lim[1] & nc_df$lon <= lon.lim[2],]
    nc_df <- nc_df[nc_df$lat >= lat.lim[1] & nc_df$lat <= lat.lim[2],]
    nc_df <- na.omit(nc_df)
    nc_df$date <- paste(i,'/',m, sep="")
    all.iasi <- rbind(all.iasi, nc_df)
  }
}
```

```{r}
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
points(nc_df$lon, nc_df$lat, cex=nc_df$nh3/max(nc_df$nh3))
```

```{r}
lons <- seq(lon.lim[1], lon.lim[2], .25)
lats <- seq(lat.lim[1]-.5, lat.lim[2], .25)
```

```{r IASI data smoother}
m.iasi <- matrix(0, nrow=length(lons), ncol=length(lats))
for (i in 1:length(lons)) {
  for (j in 1:length(lats)) {
    weights <- dnorm(all.iasi$lon, lons[i], .3) * dnorm(all.iasi$lat, lats[j], .3)
    m.iasi[i,j] <- sum(weights * all.iasi$nh3) / sum(weights)
  }
}

#write.table(m.iasi, "iasi_smooth")

```

```{r AIRS data average}
m2 <- matrix(0, nrow=length(lons), ncol=length(lats))
for (i in 1:length(lons)) {
  for (j in 1:length(lats)) {
    weights <- dnorm(all.data$lon, lons[i], .6) * dnorm(all.data$lat, lats[j], .6)
    m2[i,j] <- sum(weights * all.data$nh3) / sum(weights)
  }
}
```

```{r}
summary(lm(as.vector(m)~as.vector(m2)))


```

```{r}
image(lons, lats, m.iasi)
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
points(blob, pch=19, cex=.3)
```


```{r}
m.slope <- matrix(0, nrow=length(lons), ncol=length(lats))
for (i in 1:length(lons)) {
  for (j in 1:length(lats)) {
    if (min(sqrt((lons[i] - all.slopes$lon)^2 + (lats[j] - all.slopes$lat)^2))<1) { 
      weights <- dnorm(all.slopes$lon, lons[i], .6) * dnorm(all.slopes$lat, lats[j], .6)
      m.slope[i,j] <- sum(weights * all.slopes$slopes) / sum(weights)
    }
  }
}
```


```{r}
#m.slope2 <- m.slope
#m.slope2[which(-m.slope>max(m.slope))] <- -max(m.slope) 
par(bg="gray")
image(lons, lats, m.slope2, col=alyssa.col)
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
```

```{r}
color.bar <- function(lut, min, max=-min, nticks=11, ticks=seq(min, max, len=nticks), title='') {
    scale = (length(lut)-1)/(max-min)

    dev.new(width=1.75, height=5)
    plot(c(0,10), c(min,max), type='n', bty='n', xaxt='n', xlab='', yaxt='n', ylab='', main=title)
    axis(2, ticks, las=1)
    for (i in 1:(length(lut)-1)) {
     y = (i-1)/scale + min
     rect(0,y,10,y+1/scale, col=lut[i], border=NA)
    }
}
color.bar(colorRampPalette(c("darkgreen", "white", "darkred"))(100), -1)
color.bar(hcl.colors(100, "YlOrRd", rev = TRUE), round(min(m.slope)*10^4,1), round(max(m)*10^11,1))

color.bar(colorRampPalette(c("#2171b5", "white", "red3"))(100), -1)

alyssa.col <- colorRampPalette(c("#2171b5", "white", "red3"))(100)
```

```{r}
in.state <- function(long, lat) {
  states <- map_data("state")
  states$subregion[is.na(states$subregion)] <- "none"
  statelist <- unique(states$region)
  stated <- "none"
  while (stated=="none") {
    for (i in 1:length(statelist)) { 
      data.sub <- states[states$region == statelist[i],]
      regs <- unique(data.sub$subregion)
      for (j in 1:length(regs)) {
        if (max(data.sub$long[data.sub$subregion==regs[j]])>=long & 
            min(data.sub$long[data.sub$subregion==regs[j]])<=long &
            max(data.sub$lat[data.sub$subregion==regs[j]])>=lat & 
            min(data.sub$lat[data.sub$subregion==regs[j]])<=lat)
          stated <- statelist[i]
      }
    }
  }
  return(stated)
}


North <- 49.382808
South <-  24.521208
East <- -66.945392
West <-  -124.736342
grid1 <- seq(South, North, length.out=50)
grid2 <- seq(West, East, length.out=50)
good.grid <- c()
for (i in 1:100) {
  for (j in 1:100) {
    state <- in.state(grid1[i], grid2[j])
    if (state!="none")
      good.grid <- rbind(good.grid, c(grid1[i], grid2[j]))
  }
}

```

For computing correlation between animal density and nh3.
```{r}
nhthree <- c()
#animal_density <- read.csv("~/Downloads/animal_density.csv")
for (i in 1:nrow(animal_density)) {
  a <- which(lons==animal_density$lon[i])
  b <- which(lats==animal_density$lat[i])
  nhthree[i] <- m.iasi[a,b]
}
```

```{r levelset}
loc.temp <- cbind(animals$lat, animals$lon)
loc.temp <- cbind(loc.temp, animals$ad)
loc.temp <- as.data.frame(loc.temp)
#points(loc.temp[,2:1], cex=loc.temp[,3]/150)
names(loc.temp) <- c("lat", "lon", "ad")
an.lev <- loc.temp[loc.temp$lon < -85 & loc.temp$lon > -110 & loc.temp$lat > 30.5,]
lon.mw <- seq(min(an.lev$lon), max(an.lev$lon),.25) 
lat.mw <- seq(min(an.lev$lat), max(an.lev$lat),.25) 
grid.mw <- matrix(0, ncol=2, nrow=length(lon.mw)*length(lat.mw))
k <- 1
for (i in 1:length(lon.mw)) { 
  for (j in 1:length(lat.mw)) {
    grid.mw[k,] <- c(lon.mw[i], lat.mw[j])
    k <- k + 1
  }
}
plot(grid.mw, cex=.1)
points(an.lev[,2:1], cex=an.lev[,3]/100, col='red')
n.data <- nrow(an.lev)
n.grid <- nrow(grid.mw)
all.data <- rbind(as.matrix(an.lev[,2:1]), grid.mw) 
ds <- as.matrix(dist(all.data))
dist.mw <- ds[1:n.data, (n.data+1):(n.data+n.grid)]
weights <- dnorm(dist.mw, .5)
smooth.mw <- c()
for (i in 1:n.grid) {
  smooth.mw[i] <- sum(weights[,i] * an.lev[,3]) / sum(weights[,i])
}
image(lon.mw, lat.mw, matrix(smooth.mw * (smooth.mw > quantile(smooth.mw, .82)), nrow=length(lon.mw), byrow=TRUE))

image(lon.mw, lat.mw, matrix(smooth.mw * (smooth.mw > quantile(smooth.mw, .82)), nrow=length(lon.mw), byrow=TRUE))

plot(grid.mw[smooth.mw > quantile(smooth.mw, .82)])

mw.mat <- matrix(1*(smooth.mw > quantile(smooth.mw, .82)), nrow=length(lon.mw), byrow=TRUE)
edge <- c()
#for (i in 2:99) {
#  for (j in 2:73) {
#    if (mw.mat[i,j]==1 & var(as.vector(mw.mat[(i-1:i+1),(j-1):(j+1)]))>0)
#      edge <-c(edge, j*74 + i)
#  }
#}
edge <- c()
mw.vec <- 1*(smooth.mw > quantile(smooth.mw, .9))
for (i in 101:length(smooth.mw)) {
  if ((mw.vec[i] - mw.vec[i-1])!=0 | (mw.vec[i] - mw.vec[i-74])!=0)
    edge <- c(edge, i)
}
plot(grid.mw[edge,], cex=.3, pch=14, col="pink")

edge <- c()
mw.vec <- 1*(smooth.mw > 26)#quantile(smooth.mw, .80))
for (i in 101:length(smooth.mw)) {
  if ((mw.vec[i] - mw.vec[i-1])!=0 | (mw.vec[i] - mw.vec[i-74])!=0)
    edge <- c(edge, i)
}
points(grid.mw[edge,], cex=.3, pch=14, col='green')
```


```{r}
plot(grid.mw[smooth.mw > quantile(smooth.mw, .80),], col=alyssa.col[1])
for (i in 2:100)
  points(grid.mw[smooth.mw > quantile(smooth.mw, .80+i/500),], col=alyssa.col[i], cex=1-i/200)

```


```{r}
blobby <- read.csv("~/Downloads/blob.csv", header=FALSE)
animals <- read.csv("~/Downloads/animal_density.csv")


#blobby <- matrix(as.numeric(unlist(strsplit(blob$V2, ','))), ncol=2, byrow=TRUE)
#lats <- unique(blobby[,2])
lats1 <- unique(blobby[,2])
blobpts <- c(0,0)
for (i in 1:length(lats1)) {
  these <- range(blobby[which(blobby[,2]==lats1[i]),1])
  blobpts <- rbind(blobpts, cbind(seq(min(these), max(these), .25), lats1[i]))
}
blobpts <- blobpts[-1,]
good <- c()
for (i in 1:nrow(blobpts)) {
  if (blobpts[i,1]*2==round(blobpts[i,1]*2) & blobpts[i,2]*2==round(blobpts[i,2]*2))
    good <- c(good, i)
}
blobpts <- blobpts[good, ]

good.ad <- c()
for (i in 1:nrow(blobpts)) {
  good.ad <- c(good.ad, which(animals$lon == blobpts[i,1] & animals$lat == blobpts[i,2]))
}
good.ad <- animals$ad[good.ad]

good.iasi <- c()
for (i in 1:nrow(blobpts)) {
  good.iasi <- c(good.iasi, m[which(lons==blobpts[i,1]),which(lats==blobpts[i,2])])
}
```


```{r}
par(mfrow=c(2,1))
x <- runif(1000, 0,100)
y <- x + rnorm(1000, 0 , 20)
plot(x,y)
legend(0, 150, round(cor(x,y),3), bty='n')

forty <- which(x>40 & x<50)
plot(x[forty], y[forty])
legend(40, 100, round(cor(x[forty],y[forty]),3), bty="n")
```
```{r}
d.aqs <- c()
for (i in 1:nrow(ubercool)) {
  d.aqs[i] <- min(sqrt((aqs.locs[,1]-ubercool$lon[i])^2 + (aqs.locs[,2]-as.numeric(ubercool$lat[i]))^2))
}
hist(d.aqs)
rug(d.aqs[which(ubercool$ALL2017AU > quantile(ubercool$ALL2017AU, .98, na.rm=TRUE) & ubercool$lon > -115)], col='red', lwd=1.5)
```


Old Analysis

```{r, include=FALSE}
animals <- read.csv("ubercool.csv") 
animals <- animals[,-c(1,2)]
nh3 <- read.csv("nh3data.csv")
county.nh3 <- read.csv("counties.csv")
in.usa <- which(is.na(county.nh3[,2])==FALSE)
require(RCurl)
require(ggplot2)
```


Function to return county given (lat,lon), needed for the 5 state analysis 
```{r}
countyfromlatlon <- function(lat, lon) {
  ## 
  # Returns county for given latitude and longitude coordinates in the US
  # 
  # Inputs:
  #   lat, lon: latidude and longitude in decimal degrees
  #
  # Returns: 
  #   name of county, state, county fips code
  ##
  url <- paste("https://geo.fcc.gov/api/census/area?lat=",lat,"&lon=",lon,"&format=json", sep="")
  x <- getURL(url)
  split <- strsplit(x, '\"')[[1]]
  return(list(county=split[22], state=split[34], fips = split[18]))
}

# Sample Usage
lat = 34.0966764
lon = -117.7197785
countyfromlatlon(lat, lon)  #Claremont, CA

lat = 40.4592726
lon = -74.3609822
countyfromlatlon(lat, lon) #Sayreville, NJ
```

Here's where the level set is estimated in "CAFO alley""
```{r levelset}
loc.temp <- cbind(animals$lat, animals$lon)
loc.temp <- cbind(loc.temp, val)
loc.temp <- as.data.frame(loc.temp)
#points(loc.temp[,2:1], cex=loc.temp[,3]/150)
names(loc.temp) <- c("lat", "lon", "ad")
an.lev <- loc.temp[loc.temp$lon < -85 & loc.temp$lon > -110 & loc.temp$lat > 30.5,]
lon.mw <- seq(min(an.lev$lon), max(an.lev$lon),.25) 
lat.mw <- seq(min(an.lev$lat), max(an.lev$lat),.25) 
grid.mw <- matrix(0, ncol=2, nrow=length(lon.mw)*length(lat.mw))
k <- 1
for (i in 1:length(lon.mw)) { 
  for (j in 1:length(lat.mw)) {
    grid.mw[k,] <- c(lon.mw[i], lat.mw[j])
    k <- k + 1
  }
}
plot(grid.mw, cex=.1)
points(an.lev[,2:1], cex=an.lev[,3]/100, col='red')
n.data <- nrow(an.lev)
n.grid <- nrow(grid.mw)
all.data <- rbind(as.matrix(an.lev[,2:1]), grid.mw) 
ds <- as.matrix(dist(all.data))
dist.mw <- ds[1:n.data, (n.data+1):(n.data+n.grid)]
weights <- dnorm(dist.mw, .5)
smooth.mw <- c()
for (i in 1:n.grid) {
  smooth.mw[i] <- sum(weights[,i] * an.lev[,3]) / sum(weights[,i])
}
image(lon.mw, lat.mw, matrix(smooth.mw * (smooth.mw > quantile(smooth.mw, .82)), nrow=length(lon.mw), byrow=TRUE))

image(lon.mw, lat.mw, matrix(smooth.mw * (smooth.mw > quantile(smooth.mw, .82)), nrow=length(lon.mw), byrow=TRUE))

plot(grid.mw[smooth.mw > quantile(smooth.mw, .82),])

mw.mat <- matrix(1*(smooth.mw > quantile(smooth.mw, .82)), nrow=length(lon.mw), byrow=TRUE)
edge <- c()
#for (i in 2:99) {
#  for (j in 2:73) {
#    if (mw.mat[i,j]==1 & var(as.vector(mw.mat[(i-1:i+1),(j-1):(j+1)]))>0)
#      edge <-c(edge, j*74 + i)
#  }
#}
edge <- c()
mw.vec <- 1*(smooth.mw > quantile(smooth.mw, .82))
for (i in 101:length(smooth.mw)) {
  if ((mw.vec[i] - mw.vec[i-1])!=0 | (mw.vec[i] - mw.vec[i-74])!=0)
    edge <- c(edge, i)
}
plot(grid.mw[edge,], cex=.3, pch=14)
```

Average NH3 over range of years
```{r}
locs <- unique(nh3[c("lat", "lon")])
years <- 2012:2016#between 2002 and 2016
good <- c()  # which rows correspond to our year range
for (i in 1:nrow(nh3)) {
  good[i] <- nh3$year[i] %in% years
}
nh3.sub <- nh3[good,]
nh3.avg <- matrix(0,nrow=nrow(locs), ncol=6)
for (i in 1:nrow(locs)) {
  ammo <- c()
  for (j in 1:4) { #loop over seasons
    ammo[j] <-  mean(nh3.sub$ammo[which(nh3.sub$lat==locs[i,1] & nh3.sub$lon==locs[i,2] & nh3.sub[,5+j])], na.rm=TRUE)
  }
  nh3.avg[i,] <- unlist(c(locs[i,], ammo))
}
nh3.avg <- as.data.frame(nh3.avg)
names(nh3.avg) = c("lat", "lon", "winter", "spring", "summer", "fall")
nh3.avg <- nh3.avg[in.usa,]
assign(paste("nh3.avg", ".", min(years), ".", max(years), sep=""), nh3.avg)
```


```{r}
par(mfrow=c(2,1))
par(mar=c(0,0,0,0))

val <- animals$ALL2017AU / animals$area# - animals$ALL2007AU
good <- which(is.na(val)==FALSE)
val <- val[good]
mi <- min(val)
ma <- max(val)
colors <- heat.colors(50, rev=TRUE)
rank <- ceiling((val-mi)/(ma-mi)*49)
rank <- rank + (rank==0)
win.col <- colors[rank]
plot(animals$lon[good], animals$lat[good], col=win.col, pch=15, cex=.7)

###

val <- nh3.avg.2012.2016$summer
good <- which(is.na(val)==FALSE)
val <- val[good]
mi <- min(val)
ma <- max(val)
colors <- heat.colors(50, rev=TRUE)
rank <- ceiling((val-mi)/(ma-mi)*49)
rank <- rank + (rank==0)
win.col <- colors[rank]
plot(nh3.avg.2013.2016$lon[good], nh3.avg.2013.2016$lat[good], col=win.col, pch=15, cex=.7)
```

Run a kernel smoother
```{r}

colors <- heat.colors(55, rev=TRUE)[5:55]
par(mar=rep(0,4))
season <- "summer"
bw <- .75
data <- nh3.avg.2012.2016
data.temp <- data[complete.cases(data[c("lat", "lon", season)]),]
loc.temp <- data.temp[c("lat", "lon")]
dmat <- as.matrix(dist(data.temp[c("lat", "lon")]))
weights <- dnorm(dmat, 0, bw)
data.temp$smoothed <- (weights %*% as.matrix(data.temp[season])) / apply(weights, 1, sum)
val <- data.temp$smoothed
nh3.smooth <- val
mi <- min(val)
ma <- max(val)
rank <- ceiling((val-mi)/(ma-mi)*49)
rank <- rank + (rank==0)
win.col.nh3 <- colors[rank]
plot(data.temp$lon, data.temp$lat, col=win.col.nh3, pch=15, cex=1.7)
legend(min(data.temp$lon), min(data.temp$lat)+1, "NH3")


#  Now smooth animal density at same points
#cors <- c()

#for (i in 1:1000) {  #randomization test
var <- "ALL2017AU"
bw <- .75
data.temp <- animals[complete.cases(animals[c("lat", "lon", var, "area")]),]
#data.temp <- data.temp[data.temp[,60]!=0,]  #do I remove 0s or keep them? 
#data.temp[,60] <- sample(data.temp[,60])
n.a <- nrow(data.temp)
locs.all <- rbind(data.temp[c("lat", "lon")],loc.temp[c("lat", "lon")])
dmat <- as.matrix(dist(locs.all))
dmat.sub <- dmat[(n.a+1):nrow(dmat),1:n.a] #cross-correlation terms
weights <- dnorm(dmat.sub, 0, bw) 
#weights.thresh <- (weights > .01) 
loc.temp$smoothedAU <- weights %*% as.matrix(data.temp[var] / data.temp$area) / apply(weights, 1, sum)
#loc.temp$maxAU <- c()
#for (i in 1:nrow(weights)) {
#  loc.temp$maxAU[i] <- max(as.matrix(data.temp[var])[weights.thresh[i,]])# & data.temp$lon < locs.all$lon[i]]) #biggest location east #as.matrix(data.temp[var])[which.max(weights[i,])]#
#  if (loc.temp$maxAU[i]==-Inf)
#    loc.temp$maxAU[i] <- NA #as.matrix(data.temp[var])[which.max(weights[i,])]
#}
val <-  loc.temp$smoothedAU # loc.temp$maxAU # apply(weights, 1, sum)  # loc.temp$smoothedAU # loc.temp$smoothedAU #
au.smooth <- val
mi <- min(val, na.rm=TRUE)
ma <- max(val, na.rm=TRUE)
rank <- ceiling((val-mi)/(ma-mi)*49)
rank <- rank + (rank==0)
win.col.au <- colors[rank]
plot(loc.temp$lon, loc.temp$lat, col=win.col.au, pch=15, cex=1.7, yaxt='n')
legend(min(data.temp$lon), min(data.temp$lat)+3, c("Animal","Density"))

# var <- "popn"
# bw <- .75
# data.temp <- animals[complete.cases(animals[c("lat", "lon", var, "area")]),]
# n.a <- nrow(data.temp)
# locs.all <- rbind(data.temp[c("lat", "lon")],loc.temp[c("lat", "lon")])
# dmat <- as.matrix(dist(locs.all))
# dmat.sub <- dmat[(n.a+1):nrow(dmat),1:n.a] #cross-correlation terms
# weights <- dnorm(dmat.sub, 0, bw) 
# loc.temp$smoothedpop <- weights %*% as.matrix(data.temp[var] / data.temp$area) / apply(weights, 1, sum)
# val <-  loc.temp$smoothedpop 
# pop.smooth <- val
# mi <- min(val, na.rm=TRUE)
# ma <- max(val, na.rm=TRUE)
# rank <- ceiling((val-mi)/(ma-mi)*49)
# rank <- rank + (rank==0)
# win.col.pop <- colors[rank]
# plot(loc.temp$lon, loc.temp$lat, col=win.col.pop, pch=15, cex=1.7, yaxt='n')
# legend(min(data.temp$lon), min(data.temp$lat)+3, c("Pop'n","Density"))

### Correlation Analysis
#plot(nh3.smooth, apply(weights, 1, sum))
#dev.off()
plot(au.smooth, nh3.smooth)
abline(v=0); abline(0,0)
cor(au.smooth, nh3.smooth)
#cors[i] <- cor(au.smooth, nh3.smooth)#, method="spearman")
#}
#cor(cbind(au.smooth, nh3.smooth)[complete.cases(cbind(au.smooth, nh3.smooth)),])
#cor(au.smooth[loc.temp$lon < -78 & loc.temp$lon > -105], nh3.smooth[loc.temp$lon < -78 & loc.temp$lon > -105], method="spearman")
```
Let's put the data back on a grid (with missing grid points added back in but mapping to NA) so that we can draw a proper heat map (image() in R).

```{r}
lon.min <- min(loc.temp$lon)-5
lon.max <- max(loc.temp$lon)+5
lat.min <- min(loc.temp$lat)-5
lat.max <- max(loc.temp$lat)+5
lon.grid <- seq(lon.min, lon.max, 1)
lat.grid <- seq(lat.min, lat.max, 1)
nh3.grid <- matrix(NA, nrow=length(lon.grid), ncol=length(lat.grid)) -> au.grid
for (i in 1:length(lon.grid)) {
  for (j in 1:length(lat.grid)) {
    indec <- which(loc.temp$lon==lon.grid[i] & loc.temp$lat==lat.grid[j])
    if (length(indec) > 0) {
      nh3.grid[i,j] <- nh3.smooth[indec]
      au.grid[i,j] <- au.smooth[indec]
    }
  }
}
par(mfrow=c(1,2))
par(mar=c(0,0,0,0))
image(lon.grid, lat.grid, nh3.grid, yaxt="n", asp=1.13, xaxt="n", main="\n \n Ammonia")
lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
points(grid.mw[edge,], cex=.1, pch=14, col='green') 
image(lon.grid, lat.grid, au.grid, yaxt="n", asp=1.13, main="\n \n Animal Density")
lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
points(grid.mw[edge,], cex=.1, pch=14, col='green')
```




Run naive wind dynamics  - at 40 degrees latitude, one degree of longitude is about 53 miles.  Assuming 12 mph prevailing west to east winds means air moves 288 miles in a day, or about 5.25 degrees of longitude.  Wind dynamics of $.9^(degrees)$ are used.  
```{r}
windy <- rep(0, nrow(loc.temp))
for (i in 1:nrow(loc.temp)) {
  long <- loc.temp$lon[i]
  for (j in 0:10) {
    long.look <- long-j
    index <- which(loc.temp$lat == loc.temp$lat[i] & loc.temp$lon == long.look)
    if (length(index) > 0)
      windy[i] <- windy[i] +  .8^j * loc.temp$smoothedAU[index]
  }
}
val <- windy
mi <- min(val)
ma <- max(val)
rank <- ceiling((val-mi)/(ma-mi)*49)
rank <- rank + (rank==0)
win.col <- colors[rank]

par(mfrow=c(1,2))
par(mar=c(0,0,0,0))
image(lon.grid, lat.grid, nh3.grid, yaxt="n", asp=1)
lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
plot(loc.temp$lon, loc.temp$lat, col=win.col, pch=15, cex=1.1, yaxt='n', asp=1)
lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
plot(au.smooth[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35], nh3.smooth[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35])

plot(windy[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35], nh3.smooth[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35])


#plot(windy, nh3.smooth)

abline(v=0); abline(0,0)
#whole US wind
cor(windy, nh3.smooth)#, method="spearman")

#midwest wind
cor(windy[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35], nh3.smooth[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35])#, method="spearman")

#midwest no wind
cor(au.smooth[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35], nh3.smooth[loc.temp$lon < -87 & loc.temp$lon > -102 & loc.temp$lat < 46 & loc.temp$lat > 35])#, method="spearman")



```




```{python}
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep  6 12:36:26 2021

@author: gjc04747
"""

import datetime
from itertools import chain
import numpy as np

#cd ~/42401Data

years = (list(range(2, 20)))
for i in range(len(years)) :
    temp = str(years[i])
    if len(temp) == 1:
        temp = str(0) + temp
    file = "./42401Data/hourly_42401_20" + temp + ".csv"
    keep = [5,6,13]  #columns to keep
    summer3 = []
    x = open(file)
    y = x.readline().split(",")
    y = x.readline().strip().split(",")
    while len(y) > 5 :
        if y[10].strip('"') == "15:00" and datetime.datetime.strptime(y[9].strip('"'),"%Y-%m-%d").month in [6,7,8] :
            summer3.append([float(y[i]) for i in keep])  
        y = x.readline().strip().split(",")
    x.close()
    np.savetxt("./42401Data/so2jja"+temp+".csv", 
           summer3,
           delimiter =", ", 
           fmt ='% s')

```

We have $SO_2$ data from 2002 to 2019, and we will compare to $NH_3$ data from the same year.  We will get values of $NH_3$ based on a single year (so $n=3$ only).   For $SO_2$, we want to match sattelite data, which is recorded at 1:30, so we average the 1 and 2pm readings. 

```{r}
for (k in 2003:2016) {
  so2 <- read.csv(paste("~/42401Data/so2jja", substr(toString(k),3,4), "1pm2pm.csv", sep=""), header=FALSE)
  temp.n <- nrow(unique(so2[,1:2]))  #look at all locations
  so2avg <- matrix(0, ncol=3, nrow=temp.n)
  temp3 <- as.matrix(unique(so2[,1:2]))
  for (i in 1:temp.n) {
    temp <- temp3[i,]
    temp2 <- mean(so2[so2[,1]==temp[1] & so2[,2]==temp[2],3])
    so2avg[i,] <-  c(temp, temp2 ) #location + average over year
  }
  locsso2 <- so2avg[,1:2]  #save so2 locations
  locsnh3 <- unique(nh3[c("lat", "lon")])
  locsnh3 <- as.matrix(locsnh3[in.usa, ])  #all sat coordinates in US
  total.locs <- temp.n + nrow(locsnh3)  # total number of points
  distances <- as.matrix(dist(rbind(locsso2, locsnh3)))[1:temp.n, (temp.n+1):total.locs] #pairwise distances between towers and sat coor.
  bw <- .75  #same bandwidth as rest of analysis
  weights <- t(dnorm(distances, 0, bw))
  smoothso2 <- weights %*% as.matrix(so2avg[,3]) / apply(weights, 1, sum)
  years <- k #between 2002 and 2016
  good <- c()  # which rows correspond to our year range
  for (i in 1:nrow(nh3)) {
    good[i] <- nh3$year[i] %in% years  #find the right year
  }
  nh3.sub <- nh3[good,]  #subset nh3 to only include relevant year
  nh3.avg <- matrix(0,nrow=nrow(locsnh3), ncol=6)
  for (i in 1:nrow(locsnh3)) {
    ammo <- c()
    for (j in 1:4) { #loop over seasons
      ammo[j] <-  mean(nh3.sub$ammo[which(nh3.sub$lat==locsnh3[i,1] & nh3.sub$lon==locsnh3[i,2] & nh3.sub[,5+j])], na.rm=TRUE)
    }
    nh3.avg[i,] <- unlist(c(locsnh3[i,], ammo))
  }
  nh3.avg <- as.data.frame(nh3.avg)
  names(nh3.avg) = c("lat", "lon", "winter", "spring", "summer", "fall")
  season <- "summer"
  bw <- .75
  data <- nh3.avg
  locs.all <- c()
  data.temp <- data[complete.cases(data[c("lat", "lon", season)]),]
  loc.temp <- data.temp[c("lat", "lon")]
  dmat <- as.matrix(dist(data.temp[c("lat", "lon")]))
  weights <- dnorm(dmat, 0, bw)
  data.temp$smoothed <- (weights %*% as.matrix(data.temp[season])) / apply(weights, 1, sum)
  val <- data.temp$smoothed
  nh3.smooth <- val
  close.enuf <- which(apply(distances, 2, min)<1)
  n.close <- length(close.enuf)
  comps <- matrix(0, nrow=n.close, ncol=4)
  for (i in 1:n.close) {
    comps[i,] <- c(locsnh3[close.enuf[i],], nh3.smooth[close.enuf[i]], smoothso2[close.enuf[i]])
  }
  locs.all <- rbind(locs.all, comps[,1:2])
  assign(paste("compsso2", k, sep=""), comps)
}


locs.all <- unique(locs.all) 
all.coors <- matrix(0, nrow=nrow(locs.all), ncol=3)
for (j in 1:nrow(locs.all)) { 
  coor <- locs.all[j,]
  years <- 2003:2016
  nh3.1 <- c()
  so2.1 <- c()
  for (i in 1:length(years)) {
    year <- years[i]
    file <- get(paste("compsso2", year, sep=""))
    ind <- which(file[,1]==coor[1] & file[,2]==coor[2])
    if (length(ind) > 0) {
      nh3.1[i] <- file[ind, 3]
      so2.1[i] <- file[ind, 4]
    }
  }
  all.coors[j,] <- c(locs.all[j,], cor(nh3.1, so2.1))
}

cors <- all.coors[complete.cases(all.coors),]
signs <- cors[,3] > 0
plot(cors[,2:1], col=2+signs, cex=abs(cors[,3])*2, main="dot plot for checking")
```

```{r}
xam <- -125.5:-65.5 #seq(min(cors[,2]), max(cors[,2]), 1)
yam <- 25.5:50.5 #seq(min(cors[,1]), max(cors[,1]), 1)
colfunc1 <- colorRampPalette(c("white", "darkred"))
colfunc2 <- colorRampPalette(c("white", "darkgreen"))
cols <- c(colfunc2(20)[20:1], "white", colfunc1(20))
z <- matrix(NA, nrow=length(xam), ncol=length(yam))
for (i in 1:length(xam)) {
  for (j in 1:length(yam)) {
    if (length(which(cors[,2]==xam[i] & cors[,1]==yam[j]))>0)
      z[i,j] <- cors[which(cors[,2]==xam[i] & cors[,1]==yam[j]),3]
  }
}
image(xam, yam, z, col=cols, zlim=c(-1,1))
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
loc.temp <- cbind(animals$lat, animals$lon)
val <- animals$ALL2017AU / animals$area
loc.temp <- cbind(loc.temp, val)
loc.temp <- as.data.frame(loc.temp)
#points(loc.temp[,2:1], cex=loc.temp[,3]/200)
#points(grid.mw[smooth.mw > quantile(smooth.mw, .82),], pch=15, cex=1.3, col='lightgrey') #generated towards bottom of file
#rect(-105, 35, -78, 46, lty=3, col=NULL, border='blue', lwd=3)

lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)

points(grid.mw[edge,], cex=.3, pch=15, col='purple')
```

 To create legend for correlation heat maps (SO2/NO2)
```{r}
color.bar <- function(lut, min, max=-min, nticks=11, ticks=seq(min, max, len=nticks), title='') {
    scale = (length(lut)-1)/(max-min)

    dev.new(width=1.75, height=5)
    plot(c(0,10), c(min,max), type='n', bty='n', xaxt='n', xlab='', yaxt='n', ylab='', main=title)
    axis(2, ticks, las=1)
    for (i in 1:(length(lut)-1)) {
     y = (i-1)/scale + min
     rect(0,y,10,y+1/scale, col=lut[i], border=NA)
    }
}
color.bar(colorRampPalette(c("darkgreen", "white", "darkred"))(100), -1)
```








1:30 on the NO2 data 
```{r}
for (k in 2003:2016) {
  no2 <- read.csv(paste("~/42602Data/no2jja", substr(toString(k),3,4), "1pm2pm.csv", sep=""), header=FALSE)
  temp.n <- nrow(unique(no2[,1:2]))
  no2avg <- matrix(0, ncol=3, nrow=temp.n)
  temp3 <- as.matrix(unique(no2[,1:2]))
  for (i in 1:temp.n) {
    temp <- temp3[i,]
    temp2 <- mean(no2[no2[,1]==temp[1] & no2[,2]==temp[2],3])
    no2avg[i,] <-  c(temp, temp2 )
  }
  locsno2 <- no2avg[,1:2]
  locsnh3 <- unique(nh3[c("lat", "lon")])
  locsnh3 <- as.matrix(locsnh3[in.usa, ])
  total.locs <- temp.n + nrow(locsnh3)
  distances <- as.matrix(dist(rbind(locsno2, locsnh3)))[1:temp.n, (temp.n+1):total.locs]
  bw <- .75
  weights <- t(dnorm(distances, 0, bw))
  smoothno2 <- weights %*% as.matrix(no2avg[,3]) / apply(weights, 1, sum)
  years <- k #between 2002 and 2016
  good <- c()  # which rows correspond to our year range
  for (i in 1:nrow(nh3)) {
    good[i] <- nh3$year[i] %in% years
  }
  nh3.sub <- nh3[good,]
  nh3.avg <- matrix(0,nrow=nrow(locs), ncol=6)
  for (i in 1:nrow(locs)) {
    ammo <- c()
    for (j in 1:4) { #loop over seasons
      ammo[j] <-  mean(nh3.sub$ammo[which(nh3.sub$lat==locs[i,1] & nh3.sub$lon==locs[i,2] & nh3.sub[,5+j])], na.rm=TRUE)
    }
    nh3.avg[i,] <- unlist(c(locs[i,], ammo))
  }
  nh3.avg <- as.data.frame(nh3.avg)
  names(nh3.avg) = c("lat", "lon", "winter", "spring", "summer", "fall")
  season <- "summer"
  bw <- .75
  data <- nh3.avg
  locs.all <- c()
  data.temp <- data[complete.cases(data[c("lat", "lon", season)]),]
  loc.temp <- data.temp[c("lat", "lon")]
  dmat <- as.matrix(dist(data.temp[c("lat", "lon")]))
  weights <- dnorm(dmat, 0, bw)
  data.temp$smoothed <- (weights %*% as.matrix(data.temp[season])) / apply(weights, 1, sum)
  val <- data.temp$smoothed
  nh3.smooth <- val
  close.enuf <- which(apply(distances, 2, min)<1)
  n.close <- length(close.enuf)
  comps <- matrix(0, nrow=n.close, ncol=4)
  for (i in 1:n.close) {
    comps[i,] <- c(locsnh3[close.enuf[i],], nh3.smooth[close.enuf[i]], smoothno2[close.enuf[i]])
  }
  locs.all <- rbind(locs.all, comps[,1:2])
  assign(paste("compsno2", k, sep=""), comps)
}


locs.all <- unique(locs.all) 
all.coors <- matrix(0, nrow=nrow(locs.all), ncol=3)
for (j in 1:nrow(locs.all)) { 
  coor <- locs.all[j,]
  years <- 2003:2016
  nh3.1 <- c()
  no2.1 <- c()
  for (i in 1:length(years)) {
    year <- years[i]
    file <- get(paste("compsno2", year, sep=""))
    ind <- which(file[,1]==coor[1] & file[,2]==coor[2])
    if (length(ind) > 0) {
      nh3.1[i] <- file[ind, 3]
      no2.1[i] <- file[ind, 4]
    }
  }
  all.coors[j,] <- c(locs.all[j,], cor(nh3.1, no2.1))
}

cors <- all.coors[complete.cases(all.coors),]
signs <- cors[,3] > 0
plot(cors[,2:1], col=2+signs, cex=abs(cors[,3])*2)
```

```{r}
xam <- -125.5:-65.5 #seq(min(cors[,2]), max(cors[,2]), 1)
yam <- 25.5:50.5 #seq(min(cors[,1]), max(cors[,1]), 1)
colfunc1 <- colorRampPalette(c("white", "darkred"))
colfunc2 <- colorRampPalette(c("white", "darkgreen"))
cols <- c(colfunc2(20)[20:1], "white", colfunc1(20))
z <- matrix(NA, nrow=length(xam), ncol=length(yam))
for (i in 1:length(xam)) {
  for (j in 1:length(yam)) {
    if (length(which(cors[,2]==xam[i] & cors[,1]==yam[j]))>0)
      z[i,j] <- cors[which(cors[,2]==xam[i] & cors[,1]==yam[j]),3]
  }
}
image(xam, yam, z, col=cols, zlim=c(-1,1), main="NO2")
states <- map_data("state")
states$subregion[is.na(states$subregion)] <- "none"
statelist <- unique(states$region)
#plot(states[,1:2], t='n')
for (i in 1:length(statelist)) { 
  data.sub <- states[states$region == statelist[i],]
  regs <- unique(data.sub$subregion)
  for (j in 1:length(regs)) {
    lines(data.sub[data.sub$subregion==regs[j],1:2])
  }
}
lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)

points(grid.mw[edge,], cex=.3, pch=15, col='purple')
```





Do we not often have NO2 and SO2 data at the same sites? 

```{r}
locs.all <- c()
par(mar=c(0,0,0,0))
par(mfrow=c(5,3))
states.list <-  c("South Carolina", "Kentucky", "Ohio", "Pennsylvania", "Indiana")
for (k in 2003:2016) {
  statelist <- unique(states$region)
  plot(states[,1:2], t='n')
  for (i in 1:length(statelist)) { 
    data.sub <- states[states$region == statelist[i],]
    regs <- unique(data.sub$subregion)
    for (j in 1:length(regs)) {
      lines(data.sub[data.sub$subregion==regs[j],1:2])
    }
  }
  so2 <- read.csv(paste("~/42401Data/so2jja", substr(toString(k),3,4), "1pm2pm5states.csv", sep=""), header=FALSE)
  temp.n <- nrow(unique(so2[,1:2]))
  so2avg <- matrix(0, ncol=3, nrow=temp.n)
  temp3 <- as.matrix(unique(so2[,1:2]))
  for (i in 1:temp.n) {
    temp <- temp3[i,]  #current coordinates
    temp2 <- mean(so2[so2[,1]==temp[1] & so2[,2]==temp[2],3])
    so2avg[i,] <-  c(temp, temp2 )
  }
  points(so2avg[complete.cases(so2avg), 2:1], col="blue", cex=.5)
  locsso2 <- so2avg[,1:2]
  locsnh3 <- unique(nh3[c("lat", "lon")])
  locsnh3 <- as.matrix(locsnh3[in.usa, ])
  total.locs <- temp.n + nrow(locsnh3)
 
  distances <- as.matrix(dist(rbind(locsso2, locsnh3)))[1:temp.n, (temp.n+1):total.locs]
  bw <- .75
  weights <- t(dnorm(distances, 0, bw))
  smoothso2 <- weights %*% as.matrix(so2avg[,3]) / apply(weights, 1, sum)
  close.enuf <- apply(distances,2,min) < 1
  this.year <- cbind(locsnh3[close.enuf, 1:2], smoothso2[close.enuf])
  points(locsnh3[close.enuf,2:1], col='purple', cex=2)
  locsnh3 <- locsnh3[close.enuf,]  #only save values that are close to trailer
  years <- k #between 2002 and 2016
  good <- c()  # which rows correspond to our year range
  for (i in 1:nrow(nh3)) {
    good[i] <- nh3$year[i] %in% years  #only grab relevant year
  }
  nh3.sub <- nh3[good,]
  nh3.avg <- matrix(0,nrow=nrow(locsnh3), ncol=6)
  for (i in 1:nrow(locsnh3)) {
    ammo <- c()
    for (j in 1:4) { #loop over seasons
      ammo[j] <-  mean(nh3.sub$ammo[which(nh3.sub$lat==locsnh3[i,1] & nh3.sub$lon==locsnh3[i,2] & nh3.sub[,5+j])], na.rm=TRUE)
    }
    nh3.avg[i,] <- unlist(c(locsnh3[i,], ammo))
  }
  nh3.avg.summer <- nh3.avg[,c(1,2,5)]
  nh3.avg.summer <- as.data.frame(nh3.avg.summer)
  names(nh3.avg.summer) = c("lat", "lon", "summer")
  bw <- .75
  data <- nh3.avg.summer  #temporary call it data
  data.temp <- data[complete.cases(data),]
  loc.temp <- data.temp[,1:2]
  points(loc.temp[,2:1], cex=.3, col='green')
  dmat <- as.matrix(dist(data.temp[c("lat", "lon")]))
  weights <- dnorm(dmat, 0, bw)
  data.temp$smoothed <- (weights %*% as.matrix(data.temp[season])) / apply(weights, 1, sum)
  val <- data.temp$smoothed
  nh3.smooth <- val
  this.year <- cbind(this.year, rep(NA,nrow(this.year)))
  for (i in 1:nrow(this.year)) {
    indy <- which(this.year[i,1]==loc.temp[,1] & this.year[i,2]==loc.temp[,2])
    if (length(indy) > 0)
      this.year[i,4] <- nh3.smooth[indy]
  }
  
  
  
  
  no2 <- read.csv(paste("~/42602Data/no2jja", substr(toString(k),3,4), "1pm2pm5states.csv", sep=""), header=FALSE)
  temp.n <- nrow(unique(no2[,1:2]))
  no2avg <- matrix(0, ncol=3, nrow=temp.n)
  temp3 <- as.matrix(unique(no2[,1:2]))
  for (i in 1:temp.n) {
    temp <- temp3[i,]  #current coordinates
    temp2 <- mean(no2[no2[,1]==temp[1] & no2[,2]==temp[2],3])
    no2avg[i,] <-  c(temp, temp2 )
  }
  locsno2 <- no2avg[,1:2]
  points(no2avg[complete.cases(no2avg), 2:1], col='red')
  total.locs <- temp.n + nrow(locsnh3)
  distances <- as.matrix(dist(rbind(locsso2, locsnh3)))[1:temp.n, (temp.n+1):total.locs]
  bw <- .75
  weights <- t(dnorm(distances, 0, bw))
  smoothno2 <- weights %*% as.matrix(no2avg[,3]) / apply(weights, 1, sum)

  
  this.year <- cbind(this.year, smoothno2)
  this.year <- this.year[complete.cases(this.year),]
  this.year <- as.data.frame(this.year)
  names(this.year) <- c("lat", "lon", "so2", "nh3", "no2")
  assign(paste("comps5states", k, sep=""), this.year)
}


```

```{r}
all.data <- c()
for (k in 2003:2016) {
  data.temp <- get(paste("comps5states", k, sep=""))
  all.data <- rbind(all.data, cbind(data.temp, year=rep(k, nrow(data.temp))))
}
all.locs <- unique(all.data[,1:2])
loc.num <- c()
for (i in 1:nrow(all.data)) {
  loc.num[i] <- which(all.data[i,1]==all.locs[,1] & all.data[i,2]==all.locs[,2])
}
all.data <- cbind(all.data, location=loc.num)
bad <- which(table(loc.num)<=2)
all.data <- all.data[-which(loc.num %in% bad),] 
all.data$nh3 <- all.data$nh3 * 10^9
summary(lmer(nh3~so2+no2+(1|location), data=all.data))
#cor(all.data[all.data$location==7,5:4])
```

Check visually
```{r}
statelist <- unique(states$region)
  plot(states[,1:2], t='n')
  for (i in 1:length(statelist)) { 
    data.sub <- states[states$region == statelist[i],]
    regs <- unique(data.sub$subregion)
    for (j in 1:length(regs)) {
      lines(data.sub[data.sub$subregion==regs[j],1:2])
    }
  }
points(all.locs[,2:1])
```




Let's figure out where the improvement is coming from when wind dynamics are applied.
```{r}
fit.no.wind <- lm(nh3.smooth~au.smooth)
fit.wind <- lm(nh3.smooth ~ windy)

statelist <- unique(states$region)
  plot(states[,1:2], t='n')
  for (i in 1:length(statelist)) { 
    data.sub <- states[states$region == statelist[i],]
    regs <- unique(data.sub$subregion)
    for (j in 1:length(regs)) {
      lines(data.sub[data.sub$subregion==regs[j],1:2])
    }
  }
points(loc.temp$lon, loc.temp$lat, cex = abs(fit.no.wind$residuals) / abs(fit.wind$residuals)/40)
lines(c(-102, -87, -87, -102, -102), c(35, 35, 46, 46, 35), col="blue", lwd=3, lty=3)
points(grid.mw[edge,], cex=.3, pch=15, col='purple')
```
Lot's of improvemtn in the mid-west due east of the CAFO blob as expected.  
